{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6Dv4tquLjbV1",
        "T7WbvizWjVbl",
        "RYBfpRS1-yAs",
        "IPlDRvdHNJjE",
        "vNhEGAtBxk_z",
        "P7XfINmnDAfK",
        "Y6mne35jN5LZ",
        "_k3t-Px1Mo4D",
        "v3vtrUYGMsr0",
        "5tV0r9uAMCLl",
        "GZPBXmWNwFov",
        "lF-2boybMn8d",
        "ytUGfTCFMLsl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/DAG-Based-Tokenization/blob/parallelizing-dag-tokenizer/DAG_Text_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "1.  Include delimiter counting in suffix tree problem splitting process\n",
        "2.  Move DAG construction into the suffix tree construction process using adjacency matrices, token ID's"
      ],
      "metadata": {
        "id": "ky2XwlYkVi4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages, import libraries, set config vars"
      ],
      "metadata": {
        "id": "RWgUeFn-XoEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Liberals"
      ],
      "metadata": {
        "id": "6Dv4tquLjbV1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wpoJAKxqGmT8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "# from threading import Thread\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import unittest\n",
        "\n",
        "import math\n",
        "import random\n",
        "from queue import Queue\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import urllib.request as url\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Config"
      ],
      "metadata": {
        "id": "T7WbvizWjVbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debugging = True\n",
        "verbose = {\n",
        "    \"SuffixNode\": True,\n",
        "    \"DAGStore\": True,\n",
        "    \"FlatTreeStore\": False\n",
        "}\n",
        "\n",
        "# freq_range = range(50, 1050, 50)\n",
        "freq_range = range(2, 12, 2)\n",
        "folds = 1\n",
        "num_graphs_to_plot = 10\n",
        "max_vector_plots = 2\n",
        "tokenizations = dict()\n",
        "test_results = {\n",
        "    \"min frequency\": [],\n",
        "    \"test number\": [],\n",
        "    \"mean time\": [],\n",
        "}\n",
        "# currently only supports single characters\n",
        "delimiters = {\" \", \"\\n\"}   #r\"\\n\",     #r\"\\n\\n|.\\n|\\)\\n|:|\\.\\.\\.\",\n",
        "parallelize_suffix_tree_composition = False\n",
        "max_workers = 4"
      ],
      "metadata": {
        "id": "-cvioghyjUuG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Important functions"
      ],
      "metadata": {
        "id": "xlFwktSL-2gM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utility"
      ],
      "metadata": {
        "id": "RYBfpRS1-yAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Operational"
      ],
      "metadata": {
        "id": "IPlDRvdHNJjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_occurrences(text, delimiters=None):\n",
        "    if not delimiters:\n",
        "      delimiters = set()\n",
        "\n",
        "    # Initialize a dictionary to keep track of occurrences\n",
        "    occurrences = {s: 0 for s in delimiters}\n",
        "\n",
        "    # For each delimiter, count occurrences using regex search\n",
        "    for s in delimiters:\n",
        "        # Use re.findall to count occurrences of each delimiter in the text\n",
        "        occurrences[s] = len(re.findall(re.escape(s), text))\n",
        "\n",
        "    return occurrences"
      ],
      "metadata": {
        "id": "76cGsBYAbRoe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_regex(delimiters=None):\n",
        "    if not delimiters:\n",
        "      delimiters = set()\n",
        "\n",
        "    # Join the strings with '|' and escape special characters\n",
        "    regex_pattern = \"|\".join(re.escape(s) for s in delimiters)\n",
        "\n",
        "    # Enclose the pattern in parentheses to group it\n",
        "    return f\"({regex_pattern})\""
      ],
      "metadata": {
        "id": "JPBy4VQjZ1Q2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector Embedding"
      ],
      "metadata": {
        "id": "vNhEGAtBxk_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distances_for_subgraph(labels, adjacency_matrix, subgraph_id):\n",
        "    \"\"\"\n",
        "    Calculate Manhattan distances for nodes in a specific subgraph and update the distance matrix.\n",
        "\n",
        "    Parameters:\n",
        "    labels (numpy array): Array of subgraph labels for each node.\n",
        "    adjacency_matrix (CSR matrix): A sparse adjacency matrix of the graph.\n",
        "    subgraph_id (int): ID of the subgraph to process.\n",
        "    \"\"\"\n",
        "    # extract a list of all the subgraph vertices in the current subgraph\n",
        "    subgraph_vertices = np.where(labels == subgraph_id)[0]\n",
        "\n",
        "    n = adjacency_matrix.shape[0]\n",
        "\n",
        "    indices = []\n",
        "    values = []\n",
        "    # for each seed node\n",
        "    for i in subgraph_vertices:\n",
        "        # for each outgoing node in the subgraph\n",
        "        for x in subgraph_vertices:\n",
        "          # for each incoming node in the subgraph\n",
        "          for y in subgraph_vertices:\n",
        "              # if the entry, when double-checked, is found to be nonzero\n",
        "              if adjacency_matrix[x, y] != 0 or i == x and x == y:\n",
        "                  # calculate inverted Manhattan distance, kind of\n",
        "                  #   since a node in the same row or column as the seed\n",
        "                  #   must have a distance of 1 from the seed,\n",
        "                  #   determine the shortest distance to either the row or column\n",
        "                  #   this is a heuristic since it would take too long to re-traverse the dag from scratch\n",
        "\n",
        "                  # if the entry represents the seed node\n",
        "                  if i == x and x == y:\n",
        "                    inverse_manhattan_distance = 1\n",
        "                  else:\n",
        "                    #   numerator is 0.75 to discount weight of non-self nodes\n",
        "                    inverse_manhattan_distance = 0.75 / np.maximum(min(np.abs(x - i), np.abs(y - i)), 1)\n",
        "\n",
        "                  # Store indices and values in lists\n",
        "                  indices.append([i, x, y])\n",
        "                  values.append(inverse_manhattan_distance)\n",
        "\n",
        "    if not indices:\n",
        "        print(\"Error, the entry for the seed node was not created in the tensor\")\n",
        "        return None\n",
        "\n",
        "    # return for recombination with the entire graph's tensor\n",
        "    return indices, values"
      ],
      "metadata": {
        "id": "01mgXVqdUbBO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_adjacency_matrix(adjacency_matrix, low_mem=True):\n",
        "    \"\"\"\n",
        "    Vectorize the adjacency matrix by calculating Manhattan distances for each subgraph.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_matrix (CSR matrix): Adjacency matrix of the graph.\n",
        "\n",
        "    Returns:\n",
        "    sparse CSR matrix: 3D distance matrix.\n",
        "    \"\"\"\n",
        "    n = adjacency_matrix.shape[0]\n",
        "\n",
        "    # Identify subgraphs and their labels\n",
        "    num_subgraphs, labels = connected_components(adjacency_matrix, directed=False, return_labels=True)\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    if low_mem:\n",
        "        # Calculate distances for each subgraph in series\n",
        "        for subgraph_id in range(num_subgraphs):\n",
        "            subgraph_distance_tensor = calculate_distances_for_subgraph(labels,\\\n",
        "                                                                        adjacency_matrix,\\\n",
        "                                                                        subgraph_id)\n",
        "\n",
        "            # if there's a subgraph distance tensor to combine the original with...\n",
        "            if subgraph_distance_tensor is not None:\n",
        "                # Recombine the partial vectorizations given by the subgraphs into\n",
        "                #   a single vectorization for the entire graph\n",
        "                subgraph_indices, subgraph_values = calculate_distances_for_subgraph(labels,\n",
        "                                                                                      adjacency_matrix,\n",
        "                                                                                      subgraph_id)\n",
        "                indices = indices + subgraph_indices\n",
        "                values = values + subgraph_values\n",
        "    else:\n",
        "        # Calculate distances for each subgraph in parallel\n",
        "        # Create and start threads for each subgraph\n",
        "        threads = []\n",
        "        # for subgraph_id in range(num_subgraphs):\n",
        "        #     thread = Thread(target=calculate_distances_for_subgraph,\n",
        "        #                     args=(labels,\n",
        "        #                           adjacency_matrix,\n",
        "        #                           subgraph_id))\n",
        "        #     thread.start()\n",
        "        #     threads.append(thread)\n",
        "\n",
        "        # # Wait for all threads to complete\n",
        "        # for thread in threads:\n",
        "        #     thread.join()\n",
        "        pass\n",
        "\n",
        "\n",
        "    # print(indices, values)\n",
        "\n",
        "    # Return a sparse tensor for storing the tokens' distance tensors/embeddings\n",
        "    return tf.sparse.SparseTensor(indices=indices,\n",
        "                                  values=values,\n",
        "                                  dense_shape=[n, n, n])"
      ],
      "metadata": {
        "id": "y7oPMU_HPRds"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_array(tensor):\n",
        "  return tf.sparse.to_dense(tensor).numpy()"
      ],
      "metadata": {
        "id": "utf7-Khb9C4C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_slice(tensor, slice_index):\n",
        "  # print(\"Getting slice for token \", slice_index)\n",
        "  indices = tensor.indices.numpy()\n",
        "  values = tensor.values.numpy()\n",
        "  n = tensor.dense_shape.numpy()[0]\n",
        "  new_indices = []\n",
        "  new_values = []\n",
        "\n",
        "  for i, index in enumerate(indices):\n",
        "    if index[0] == slice_index:\n",
        "      new_indices.append([index[1], index[2]])\n",
        "      new_values.append(values[i])\n",
        "\n",
        "  # print(new_indices, new_values)\n",
        "  return tf.sparse.SparseTensor(indices=new_indices,\n",
        "                                  values=new_values,\n",
        "                                  dense_shape=[n, n])"
      ],
      "metadata": {
        "id": "fxPyCBLD2ssj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(adjacency_matrix, reversed_token_map, token_set):\n",
        "  n = len(token_set)\n",
        "  token_tensor = vectorize_adjacency_matrix(adjacency_matrix)\n",
        "\n",
        "  # create a dictionary of all the tokens and their respective tensor embedding slices\n",
        "  # print(token_tensor)\n",
        "  token_vector_mappings = {reversed_token_map[i]: get_tensor_slice(token_tensor, i) for i in range(n)}\n",
        "\n",
        "  for token in token_set:\n",
        "      tok_vect_tensor = token_vector_mappings[token]\n",
        "\n",
        "  # print(token_vector_mappings)\n",
        "\n",
        "  return token_vector_mappings"
      ],
      "metadata": {
        "id": "V2bv4PySphAC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting and Printing"
      ],
      "metadata": {
        "id": "P7XfINmnDAfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings(embeddings, max_plots):\n",
        "  if max_plots < 1:\n",
        "    return\n",
        "\n",
        "  i = 0\n",
        "  for token, vector in embeddings.items():\n",
        "    if (i % max(len(embeddings) * int(i/max_plots), 1) == 0):\n",
        "        print(\"Embedding for'\" + token + \"':\")\n",
        "        dense_vector = tensor_to_array(vector)\n",
        "        # print(\"embedding string: \", vector)\n",
        "\n",
        "\n",
        "        # Convert the vector to its dense representation and create the heatmap\n",
        "        sns.heatmap(dense_vector, annot=False, cmap='viridis')\n",
        "\n",
        "        # Display the heatmap\n",
        "        plt.show()\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "d7I3OmAALAiF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dag(dag_store, A=None, scaling=50, edge_width=1, k=2):\n",
        "    print(\"Adjacency matrix:\\n\", A)\n",
        "\n",
        "    if A is None:\n",
        "        print(\"Adjacency matrix was empty/not defined\")\n",
        "\n",
        "    # Create a directed graph from the adjacency matrix\n",
        "    G = nx.from_numpy_array(A, create_using=nx.DiGraph)\n",
        "\n",
        "    # Relabel the token nodes\n",
        "    G = nx.relabel_nodes(G, dag_store.reversed_token_map)\n",
        "\n",
        "    # # Draw the graph without edge labels\n",
        "    # # Convert nodes to strings before calculating length\n",
        "    # nx.draw(G, with_labels=True, node_size=[scaling * len(str(node)) for node in G.nodes()],\n",
        "    #         width=edge_width, font_size=8)\n",
        "\n",
        "\n",
        "    # Calculate figure size based on the number of nodes\n",
        "    num_nodes = len(G.nodes)\n",
        "    num_edges = len(G.edges)\n",
        "    graph_size = (num_nodes) + (2 * num_edges)\n",
        "\n",
        "    figsize = graph_size * scaling/300\n",
        "    font_size = 2 + math.sqrt(scaling)/5\n",
        "\n",
        "    # Position nodes using the spring layout\n",
        "    pos = nx.spring_layout(G, seed=42, k=k/num_nodes)\n",
        "\n",
        "    # Calculate node sizes based on the length of the token text\n",
        "    node_sizes = [scaling * len(node) for node in G.nodes()]\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize), dpi=300)\n",
        "\n",
        "    # Draw nodes with sizes proportional to the length of their text\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes)\n",
        "\n",
        "    # Draw edges with widths based on edge weights\n",
        "    nx.draw_networkx_edges(G,\n",
        "                          pos,\n",
        "                          width=edge_width,\n",
        "                          arrowstyle='-|>',\n",
        "                          connectionstyle=\"arc3,rad=0.2\")\n",
        "\n",
        "    # Draw node labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=font_size, font_family=\"sans-serif\")\n",
        "\n",
        "    # # Draw edge weight labels\n",
        "    # edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "    # nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "    # Customize and show plot\n",
        "    ax = plt.gca()\n",
        "    ax.margins(0.08)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sHXlkuoDNxEd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes"
      ],
      "metadata": {
        "id": "gp8TIrvQG243"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage"
      ],
      "metadata": {
        "id": "Y6mne35jN5LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DAGStore:\n",
        "  def __init__(self,\n",
        "               vertices=None,\n",
        "               edge_set=None,\n",
        "               token_index_map=None,\n",
        "               adjacency_matrix=None):\n",
        "\n",
        "      if vertices is None:\n",
        "        vertices = dict()\n",
        "      if edge_set is None:\n",
        "        edge_set = set()\n",
        "\n",
        "      self.vertices = vertices\n",
        "      self.edge_set = edge_set\n",
        "      self.token_index_map = token_index_map\n",
        "      self.reversed_token_map = None\n",
        "      # first dimension = outgoing token node\n",
        "      # second dimension = incoming token node\n",
        "      self.adjacency_matrix = adjacency_matrix"
      ],
      "metadata": {
        "id": "MBQh8GVFOCOi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlatTreeStore:\n",
        "    def __init__(self, child_dict=None, root=None):\n",
        "\n",
        "        # a flattened tree of nodes,\n",
        "        #   so that a node's tree membership can be checked more easily\n",
        "        if child_dict is None:\n",
        "            child_dict = dict()\n",
        "        self.child_dict = child_dict\n",
        "        self.root = root\n",
        "\n",
        "\n",
        "    def tokenize(self, text, max_token_len):\n",
        "        if self.root is None:\n",
        "          raise ValueError(\"No root node provided to FlatTreeStore object\")\n",
        "\n",
        "        if debugging:\n",
        "          print(f\"Tokenizing {text}\")\n",
        "\n",
        "        tokenization = []\n",
        "        current_node = self.root\n",
        "        i = 0\n",
        "        j = 1\n",
        "\n",
        "\n",
        "        # add the token to the tokenization list\n",
        "        def add_token():\n",
        "          nonlocal i, j, current_node, tokenization\n",
        "          tokenization.append(current_node.token)\n",
        "          i = j - 1\n",
        "          j = i + 1\n",
        "          current_node = self.root\n",
        "\n",
        "\n",
        "        # while the end of the text hasn't been reached\n",
        "        #   and the max token length hasn't been reached\n",
        "        while j < len(text)+1:\n",
        "          current_token = text[i:j]\n",
        "          if debugging and verbose[\"FlatTreeStore\"]:\n",
        "            print(f\"Current token: {current_token}\")\n",
        "\n",
        "          # if the max token size is reached, store this token and move on\n",
        "          if j - i > max_token_len:\n",
        "\n",
        "            if debugging and verbose[\"FlatTreeStore\"]:\n",
        "              print(f\"Adding current token to tokenization list...\")\n",
        "\n",
        "            add_token()\n",
        "\n",
        "          # otherwise, get the next largest token\n",
        "          else:\n",
        "            #if the current token is a child of the current_node\n",
        "            if current_token in current_node.keys_to_my_children:\n",
        "                current_node = current_node.flat_tree_store.child_dict[current_token]\n",
        "                j += 1\n",
        "            # otherwise, if the current token isn't in the token set,\n",
        "            #   add the current node's token to the tokenization,\n",
        "            #   get the new text as 1 character before this issue\n",
        "            #   and set the current node to the root\n",
        "            else:\n",
        "                if debugging and verbose[\"FlatTreeStore\"]:\n",
        "                  print(f\"No child match found, adding previous token to tokenization list...\")\n",
        "\n",
        "                add_token()\n",
        "\n",
        "        if len(current_token) > 0:\n",
        "          if debugging and verbose[\"FlatTreeStore\"]:\n",
        "              print(f\"Adding last token to tokenization list...\")\n",
        "          tokenization.append(current_node.token)\n",
        "\n",
        "        if debugging:\n",
        "          print(tokenization)\n",
        "          print()\n",
        "\n",
        "        return tokenization"
      ],
      "metadata": {
        "id": "qhqOm0Nd8Kib"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SuffixNode Class\n",
        "Let:\n",
        "*    n=text length\n",
        "*    b=mean block size\n",
        "*    l=mean inter-block lexical diversity (l = n/b if every block has a unique suffix tree and l < n/b if some blocks have shared suffix tree compositions)\n",
        "\n",
        "<br>\n",
        "\n",
        "Blocked Ukkonen time complexity: O(n)\n",
        "*   If split with preprocessing (O(n) time), parallelization in a divide-and-conquer approach would offer maximum time complexity of O(b*l)\n",
        "\n",
        "\n",
        "\n",
        "pruning time complexity: O(log(b*l))\n",
        "\n",
        "tokenization time complexity: O(log(b*l))"
      ],
      "metadata": {
        "id": "_k3t-Px1Mo4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SuffixNode:\n",
        "    def __init__(self,\n",
        "                 suffix=None,\n",
        "                 token=None,\n",
        "                 frequency=0,\n",
        "                 parent=None,\n",
        "                 keys_to_my_children=None,\n",
        "                 flat_tree_store=None):\n",
        "        self.suffix = suffix\n",
        "        self.token = token\n",
        "        self.frequency = frequency\n",
        "\n",
        "        self.parent = parent\n",
        "\n",
        "        # store tokens representing keys to children in the child_dict\n",
        "        if keys_to_my_children is None:\n",
        "            keys_to_my_children = set()\n",
        "        self.keys_to_my_children = keys_to_my_children\n",
        "\n",
        "        # store the child_dict separately\n",
        "        if flat_tree_store is None:\n",
        "            flat_tree_store = FlatTreeStore()\n",
        "        self.flat_tree_store = flat_tree_store\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.token\n",
        "\n",
        "\n",
        "    def print_tree(self, indent=0):\n",
        "      my_children = {self.flat_tree_store.child_dict[key] for key in self.keys_to_my_children}\n",
        "      print(my_children)\n",
        "\n",
        "      # Iterate over the child.suffixes (features) in the tree\n",
        "      for child in my_children:\n",
        "          print(' ' * indent + str(child.suffix) + \": \" + str(child.frequency))\n",
        "          # If the child is a SuffixNode, recursively print the subtree\n",
        "          if isinstance(child, SuffixNode):\n",
        "              child.print_tree(indent=indent + 4)\n",
        "          else:\n",
        "              print(' ' * (indent + 4) + \"\\\"\" + str(child.suffix) + \"\\\": \" + str(child.frequency))\n",
        "\n",
        "\n",
        "    def set_token(self):\n",
        "      if self.parent.token:\n",
        "        self.token =  self.parent.token + self.suffix\n",
        "      else:\n",
        "        self.token = self.suffix\n",
        "\n",
        "\n",
        "    def get_split_children(self, child, split_node, new_suffix, old_suffix):\n",
        "        # Create the old child (original suffix) under the split node\n",
        "        old_child = SuffixNode(suffix=old_suffix,\n",
        "                              frequency=child.frequency,\n",
        "                              parent=split_node,\n",
        "                              flat_tree_store=self.flat_tree_store,\n",
        "                              keys_to_my_children=child.keys_to_my_children)\n",
        "        old_child.set_token()\n",
        "\n",
        "        # Create the new child (new suffix) under the split node\n",
        "        new_child = SuffixNode(suffix=new_suffix,\n",
        "                              frequency=1,\n",
        "                              parent=split_node,\n",
        "                              flat_tree_store=self.flat_tree_store)\n",
        "        new_child.set_token()\n",
        "\n",
        "        return new_child, old_child\n",
        "\n",
        "\n",
        "    def update_flat_tree_store(self, child):\n",
        "        # copy, then merge the flat_tree_store dictionaries\n",
        "        temp_dict = self.flat_tree_store.child_dict.copy()\n",
        "        temp_dict[child.token] = child\n",
        "        self.flat_tree_store = FlatTreeStore(\n",
        "                                            child_dict=self.flat_tree_store.child_dict.update(temp_dict),\n",
        "                                            root=self.flat_tree_store.root\n",
        "        )\n",
        "\n",
        "        # Ensure the child's flat_tree_store is always the same as self's\n",
        "        child.flat_tree_store = self.flat_tree_store\n",
        "\n",
        "        self.flat_tree_store.child_dict[child.token] = child\n",
        "\n",
        "\n",
        "    def split_edge(self, child, split_index, suffix):\n",
        "        # Remove the previous/original child from current node's children\n",
        "        self.keys_to_my_children.remove(child.token)\n",
        "        del self.flat_tree_store.child_dict[child.token]\n",
        "\n",
        "        # Calculate suffixes for the split\n",
        "        old_suffix = child.suffix[split_index:]\n",
        "        new_suffix = suffix[split_index:]\n",
        "\n",
        "        # Create the split node with the matching part of the suffix\n",
        "        split_node = SuffixNode(suffix=child.suffix[:split_index],\n",
        "                                frequency=child.frequency,\n",
        "                                parent=self,\n",
        "                                flat_tree_store=self.flat_tree_store)\n",
        "        split_node.set_token()\n",
        "\n",
        "        # Add the split node to the current node's children\n",
        "        self.keys_to_my_children.add(split_node.token)\n",
        "        self.flat_tree_store.child_dict[split_node.token] = split_node\n",
        "\n",
        "        new_child, old_child = self.get_split_children(child, split_node, new_suffix, old_suffix)\n",
        "\n",
        "        # Update children of the old split node\n",
        "        for grandchild_key in child.keys_to_my_children:\n",
        "            grandchild = self.flat_tree_store.child_dict[grandchild_key]\n",
        "            grandchild.parent = old_child\n",
        "            old_child.keys_to_my_children.add(grandchild_key)\n",
        "\n",
        "        # Add the children's token sets to that of its parent\n",
        "        # Combine and update the child and parent token sets\n",
        "        self.update_flat_tree_store(child=new_child)\n",
        "        self.update_flat_tree_store(child=old_child)\n",
        "\n",
        "        print(f\"New child: {new_child}\\nOld child: {old_child}\")\n",
        "\n",
        "        # Set up the split_node's children\n",
        "        split_node.keys_to_my_children = {new_child.token, old_child.token}\n",
        "        split_node.flat_tree_store = self.flat_tree_store\n",
        "        self.flat_tree_store.child_dict[split_node.token] = split_node\n",
        "\n",
        "\n",
        "    def add_suffix(self, suffix):\n",
        "\n",
        "        if debugging and verbose[\"SuffixNode\"]:\n",
        "            print(f\"Current suffix: '{suffix}'\")\n",
        "            # print(f\"Current token set: {self.get_tokens()}\")\n",
        "            print(f\"Children: {self.keys_to_my_children}\")\n",
        "\n",
        "        # Iterate over each child to find the longest common prefix\n",
        "        for child_token in self.keys_to_my_children:\n",
        "            child = self.flat_tree_store.child_dict[child_token]\n",
        "\n",
        "            # Find the longest common prefix\n",
        "            min_len = min(len(suffix), len(child.suffix))\n",
        "            i = 0\n",
        "            while i < min_len and suffix[i] == child.suffix[i]:\n",
        "                i += 1\n",
        "\n",
        "            # If there is a common prefix\n",
        "            if i > 0:\n",
        "                if debugging and verbose[\"SuffixNode\"]:\n",
        "                    print(f\"Child with shared suffix: '{child.token}'\")\n",
        "                # Update the frequency of the child node\n",
        "                child.frequency += 1\n",
        "\n",
        "                # If the common prefix matches the entire child suffix,\n",
        "                #   and the new suffix would be non-empty,\n",
        "                #   recurse on the remainder of the suffix\n",
        "                if i == len(child.suffix) and len(suffix[i:]) > 0:\n",
        "                    child.add_suffix(suffix[i:])\n",
        "\n",
        "                    # Combine and update the child and parent token sets\n",
        "                    self.update_flat_tree_store(child=child)\n",
        "\n",
        "                # Otherwise, split the edge\n",
        "                elif i < len(suffix):\n",
        "                    if debugging and verbose[\"SuffixNode\"]:\n",
        "                        print(f\"Splitting edge...\")\n",
        "                    self.split_edge(child, i, suffix)\n",
        "\n",
        "                return\n",
        "\n",
        "        # No matching suffix, create a new child\n",
        "        if debugging and verbose[\"SuffixNode\"]:\n",
        "            print(\"Creating new child...\\n\")\n",
        "        new_child = SuffixNode(suffix=suffix,\n",
        "                              frequency=1,\n",
        "                              parent=self,\n",
        "                              flat_tree_store=self.flat_tree_store)\n",
        "        new_child.set_token()\n",
        "\n",
        "        if debugging and verbose[\"SuffixNode\"]:\n",
        "            print(f\"Child token set: {new_child.get_tokens()}\")\n",
        "\n",
        "        # Combine and update the child and parent token sets\n",
        "        self.update_flat_tree_store(child=new_child)\n",
        "        self.print_tree()\n",
        "\n",
        "        if debugging and verbose[\"SuffixNode\"]:\n",
        "            print(f\"Child token set after adding child to set: {new_child.get_tokens()}\")\n",
        "            print()\n",
        "            print()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    # add the delimiter frequencies back into the suffix tree's storage\n",
        "    def add_delimiters_to_tree(self, text=\"\", delimiters=None):\n",
        "        # # WARNING: THIS DEFEATS THE PURPOSE OF THE PARALLELIZATION, BRINGS T(n) TO O(n)\n",
        "        # #   FIX BY INCLUDING DELIMITER COUNTING IN SUBTREES\n",
        "        # delimiter_counts = count_occurrences(text, delimiters)\n",
        "        delimiter_counts = {delimiter: 1 for delimiter in delimiters}\n",
        "\n",
        "        self.keys_to_my_children.update(set(delimiter_counts.keys()))\n",
        "        self.flat_tree_store.child_dict.update({\n",
        "            key: SuffixNode(suffix=key,\n",
        "                              token=key,\n",
        "                              frequency=value,\n",
        "                              parent=self,\n",
        "                              flat_tree_store=self.flat_tree_store)\\\n",
        "            for key, value in delimiter_counts.items()\n",
        "        })\n",
        "\n",
        "\n",
        "    def base_build_tree(self, text=\"\", delimiters=None, delimiter_regex=r\"\\n\"):\n",
        "        # split the text into blocks, where each block is an independent clause\n",
        "        clauses = re.split(delimiter_regex, text)\n",
        "        # print(set(clauses))\n",
        "\n",
        "        children = {SuffixNode(suffix=unique_char,\n",
        "                               token=unique_char,\n",
        "                               parent=self) for unique_char in set(text)}\n",
        "        self.keys_to_my_children = {child.token for child in children}\n",
        "        self.flat_tree_store.child_dict = {child.token: child for child in children}\n",
        "\n",
        "        if debugging and verbose[\"SuffixNode\"]:\n",
        "            print(\"Initial suffix tree (just alphabet):\")\n",
        "            self.print_tree()\n",
        "\n",
        "        for string in clauses:\n",
        "            if debugging:\n",
        "                print(f\"Building suffix tree for '{string}'...\")\n",
        "\n",
        "            # loop through the string, starting with the last character\n",
        "            for i in range(0, len(string)):\n",
        "                suffix = string[len(string) - i - 1:]\n",
        "\n",
        "                # add the suffix to the tree\n",
        "                self.add_suffix(suffix)\n",
        "\n",
        "                # if debugging and verbose[\"SuffixNode\"]:\n",
        "                #   self.print_tree()\n",
        "                #   print()\n",
        "                #   print()\n",
        "\n",
        "        self.add_delimiters_to_tree(text=text,\n",
        "                                    delimiters=delimiters)\n",
        "\n",
        "\n",
        "    def find_split_point(text, delimiters):\n",
        "        if not delimiters:\n",
        "            return len(text) // 2\n",
        "\n",
        "        # Find all matches for the delimiter regex\n",
        "        matches = list(re.finditer(delimiters, text))\n",
        "\n",
        "        if not matches:\n",
        "            return None\n",
        "\n",
        "        # Find the match closest to the middle of the string\n",
        "        mid = len(text) // 2\n",
        "        best_match = min(matches, key=lambda m: abs(m.start() - mid))\n",
        "\n",
        "        return best_match.start()\n",
        "\n",
        "\n",
        "    def merge_trees(self, other):\n",
        "        other_tree = other.flat_tree_store.child_dict\n",
        "\n",
        "        shared_tokens = set(self.keys_to_my_children).intersection(set(other.keys_to_my_children))\n",
        "        other_unique_tokens = set(other.keys_to_my_children).difference(set(self.keys_to_my_children))\n",
        "\n",
        "        # combine children held in common between both suffix trees\n",
        "        for token in shared_tokens:\n",
        "            my_child = self.flat_tree_store.child_dict[token]\n",
        "            other_child = other_tree[token]\n",
        "\n",
        "            # combine the childrens' frequencies\n",
        "            self.flat_tree_store.child_dict[token].frequency += other_child.frequency\n",
        "\n",
        "            # recursively merge any grandchildren that might be held in common\n",
        "            self.flat_tree_store.child_dict[token].merge_trees(other_child)\n",
        "\n",
        "        # add the other tree's unique children to this tree\n",
        "        for token in other_unique_tokens:\n",
        "            other_child = other_tree[token]\n",
        "\n",
        "            other_child.parent = self\n",
        "            self.flat_tree_store.child_dict[other_child.token] = other_child\n",
        "\n",
        "        # new_child_dict = self.flat_tree_store.child_dict.copy()\n",
        "        # other_tree = other.flat_tree_store.child_dict\n",
        "\n",
        "        # shared_tokens = set(self.keys_to_my_children).intersection(set(other.keys_to_my_children))\n",
        "        # other_unique_tokens = set(other.keys_to_my_children).difference(set(self.keys_to_my_children))\n",
        "\n",
        "        # for token in shared_tokens:\n",
        "        #     my_child = new_child_dict[token]\n",
        "        #     other_child = other_tree[token]\n",
        "\n",
        "        #     new_child = SuffixNode(\n",
        "        #         suffix=my_child.suffix,\n",
        "        #         token=my_child.token,\n",
        "        #         frequency=my_child.frequency + other_child.frequency,\n",
        "        #         parent=my_child.parent,\n",
        "        #         keys_to_my_children=my_child.keys_to_my_children.copy(),\n",
        "        #         flat_tree_store=FlatTreeStore(child_dict=my_child.flat_tree_store.child_dict.copy())\n",
        "        #     )\n",
        "        #     new_child.merge_trees(other_child)\n",
        "        #     new_child_dict[token] = new_child\n",
        "\n",
        "        # for token in other_unique_tokens:\n",
        "        #     other_child = other_tree[token]\n",
        "        #     new_child = SuffixNode(\n",
        "        #         suffix=other_child.suffix,\n",
        "        #         token=other_child.token,\n",
        "        #         frequency=other_child.frequency,\n",
        "        #         parent=self,\n",
        "        #         keys_to_my_children=other_child.keys_to_my_children.copy(),\n",
        "        #         flat_tree_store=FlatTreeStore(child_dict=other_child.flat_tree_store.child_dict.copy())\n",
        "        #     )\n",
        "        #     new_child_dict[token] = new_child\n",
        "\n",
        "        # self.flat_tree_store = FlatTreeStore(child_dict=new_child_dict)\n",
        "        # self.keys_to_my_children = set(new_child_dict.keys())\n",
        "\n",
        "\n",
        "    def parallelized_build_tree(text, delimiters=None, delimiter_regex=r\"\\n\"):\n",
        "        if len(text) == 0:\n",
        "            return SuffixNode()\n",
        "\n",
        "        # Find the best split point using delimiters\n",
        "        split_point = SuffixNode.find_split_point(text, delimiter_regex)\n",
        "\n",
        "        if debugging:\n",
        "          print(f\"Split text: {[text[:split_point], text[split_point:]]}\")\n",
        "\n",
        "        # if the text has been split down to the size of a word\n",
        "        if split_point == None or split_point == 0 or split_point == len(text)-1:\n",
        "            if debugging and verbose[\"SuffixNode\"]:\n",
        "                print(\"Leaf detected...\")\n",
        "            root = SuffixNode()\n",
        "            root.flat_tree_store.root = root\n",
        "\n",
        "            if debugging and verbose[\"SuffixNode\"]:\n",
        "                print(\"Initial suffix tree (just alphabet):\")\n",
        "                root.print_tree()\n",
        "\n",
        "            if debugging:\n",
        "                print(f\"Building suffix tree for '{text}'...\")\n",
        "\n",
        "            # loop through the string, starting with the last character\n",
        "            for i in range(0, len(text)):\n",
        "                suffix = text[len(text) - i - 1:]\n",
        "\n",
        "                # add the suffix to the tree\n",
        "                root.add_suffix(suffix)\n",
        "\n",
        "                if debugging and verbose[\"SuffixNode\"]:\n",
        "                  root.print_tree()\n",
        "                  print()\n",
        "\n",
        "            return root\n",
        "\n",
        "        left_half = text[:split_point]\n",
        "        right_half = text[split_point+1:]\n",
        "\n",
        "        print(left_half)\n",
        "\n",
        "        left_tree = SuffixNode.parallelized_build_tree(left_half,\n",
        "                                                       delimiters=delimiters,\n",
        "                                                       delimiter_regex=delimiter_regex)\n",
        "        right_tree = SuffixNode.parallelized_build_tree(right_half,\n",
        "                                                        delimiters=delimiters,\n",
        "                                                        delimiter_regex=delimiter_regex)\n",
        "\n",
        "        left_tree.merge_trees(right_tree)\n",
        "\n",
        "        left_tree.add_delimiters_to_tree(text=text,\n",
        "                                    delimiters=delimiters)\n",
        "\n",
        "        print()\n",
        "\n",
        "        return left_tree\n",
        "        # with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        #     future_left = executor.submit(SuffixNode.parallelized_build_tree, left_half, delimiters, delimiter_regex)\n",
        "        #     future_right = executor.submit(SuffixNode.parallelized_build_tree, right_half, delimiters, delimiter_regex)\n",
        "\n",
        "        #     left_tree = future_left.result()\n",
        "        #     right_tree = future_right.result()\n",
        "\n",
        "        # merged_tree = SuffixNode()\n",
        "        # merged_tree.merge_trees(left_tree)\n",
        "        # merged_tree.merge_trees(right_tree)\n",
        "\n",
        "        # merged_tree.add_delimiters_to_tree(text=text, delimiters=delimiters)\n",
        "\n",
        "        # print()\n",
        "\n",
        "        # return merged_tree\n",
        "\n",
        "\n",
        "    def prune_tree(self, threshold=2):\n",
        "        # If the node has no children (ie it's a leaf), return\n",
        "        if not self.keys_to_my_children:\n",
        "            # print(\"no children\")\n",
        "            return\n",
        "\n",
        "        children_to_kill = set()\n",
        "        # Recursively prune the tree\n",
        "        for child_token in self.keys_to_my_children:\n",
        "            child = self.flat_tree_store.child_dict[child_token]\n",
        "\n",
        "            if isinstance(child, SuffixNode):\n",
        "                # if the child is above the threshold or it's a single character token node\n",
        "                if child.frequency >= threshold or child.parent.token is None:\n",
        "                    # print(\"not removed\")\n",
        "                    self.flat_tree_store.child_dict[child_token].prune_tree(threshold)\n",
        "                else:\n",
        "                    # print(\"removed\")\n",
        "                    children_to_kill.add(child_token)\n",
        "\n",
        "        for child_token in children_to_kill:\n",
        "            # if the token's frequency falls below the threshold, prune it\n",
        "            self.keys_to_my_children.remove(child_token)\n",
        "            del self.flat_tree_store.child_dict[child_token]\n",
        "\n",
        "\n",
        "    # return an aggregated set of all the tokens\n",
        "    def get_tokens(self, prev_token=\"\"):\n",
        "        tokens = set(self\\\n",
        "                     .flat_tree_store\\\n",
        "                     .child_dict\\\n",
        "                     .keys())\n",
        "\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "rovuxbqdXhDu"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suffix_tree(text,\n",
        "                      threshold,\n",
        "                      delimiters=None,\n",
        "                      tree=None,\n",
        "                      parallelize=True):\n",
        "  # use the tree option in case a previous tree is to be pruned further\n",
        "  suffix_tree = tree\n",
        "  if suffix_tree is None:\n",
        "    print(\"Building modified suffix tree...\")\n",
        "\n",
        "    delimiter_regex = compile_regex(delimiters)\n",
        "\n",
        "    if parallelize:\n",
        "      print(\"Running SuffixNode.parallelized_build_tree()...\")\n",
        "      suffix_tree = SuffixNode.parallelized_build_tree(text,\n",
        "                                                       delimiters=delimiters,\n",
        "                                                       delimiter_regex=delimiter_regex)\n",
        "      # set the root of the flat tree store to the initial SuffixNode pointing to it\n",
        "      suffix_tree.flat_tree_store.root = suffix_tree\n",
        "\n",
        "    else:\n",
        "      print(\"Running suffix_tree.base_build_tree()...\")\n",
        "      # create a store for the tree nodes\n",
        "      flat_tree_store = FlatTreeStore(child_dict={\n",
        "          letter: SuffixNode(suffix=letter, token=letter) for letter in set(text)\n",
        "      })\n",
        "\n",
        "      # point all the internal tree stores back at the main one\n",
        "      for key in flat_tree_store.child_dict.keys():\n",
        "        flat_tree_store.child_dict[key].flat_tree_store = flat_tree_store\n",
        "\n",
        "      suffix_tree = SuffixNode(flat_tree_store=flat_tree_store,\n",
        "                               keys_to_my_children=set(flat_tree_store.child_dict.keys()))\n",
        "      # set the root of the flat tree store to the initial SuffixNode pointing to it\n",
        "      suffix_tree.flat_tree_store.root = suffix_tree\n",
        "\n",
        "      suffix_tree.base_build_tree(text,\n",
        "                                  delimiters=delimiters,\n",
        "                                  delimiter_regex=delimiter_regex)\n",
        "\n",
        "\n",
        "    if debugging and verbose[\"SuffixNode\"]:\n",
        "      suffix_tree.print_tree()\n",
        "      print(f\"\\nToken set: {suffix_tree.get_tokens()}\")\n",
        "\n",
        "  print(\"Pruning modified suffix tree...\")\n",
        "  suffix_tree.prune_tree(threshold=threshold)\n",
        "  if debugging:\n",
        "      suffix_tree.print_tree()\n",
        "\n",
        "  print(\"Getting token set...\")\n",
        "  tokens = suffix_tree.get_tokens()\n",
        "  if debugging:\n",
        "      print(tokens)\n",
        "\n",
        "  return suffix_tree, tokens"
      ],
      "metadata": {
        "id": "FHLSheWoyikQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CompositionDAGNode\n",
        "construction time complexity: O(log(|V| + |E|))"
      ],
      "metadata": {
        "id": "v3vtrUYGMsr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CompositionDAGNode:\n",
        "    def __init__(self,\n",
        "                 token=None,\n",
        "                 frequency=0,\n",
        "                 parents=None,\n",
        "                 flat_tree_store=None,\n",
        "                 dag_store=None):\n",
        "        self.token = token\n",
        "        self.frequency = frequency\n",
        "\n",
        "        if parents is None:\n",
        "            parents = list()\n",
        "        self.parents = parents\n",
        "\n",
        "        if flat_tree_store is None:\n",
        "            flat_tree_store = FlatTreeStore()\n",
        "        self.flat_tree_store = flat_tree_store\n",
        "\n",
        "        if dag_store is None:\n",
        "            dag_store = DAGStore()\n",
        "        self.dag_store = dag_store\n",
        "\n",
        "    def __str__(self):\n",
        "        child_tokens = {child.token for child in self.flat_tree_store.child_dict.values()}\n",
        "        parent_tokens = {parent.token for parent in self.parents}\n",
        "        return f\"Token: {self.token}\\nParents: {parent_tokens}\\nChildren: {child_tokens}\"\n",
        "\n",
        "    # make an edge between the current token and a successor\n",
        "    def add_edge(self, child):\n",
        "        # Add the larger token as a child of this token\n",
        "        # print(self.flat_tree_store.child_dict)\n",
        "        self.flat_tree_store.child_dict[child.token] = child\n",
        "\n",
        "        # Add this token to the list of parents composing the larger token\n",
        "        child.parents.append(self)\n",
        "\n",
        "        # Add an edge to the edge list, using the current token's position\n",
        "        #   in the child token as the edge weight\n",
        "        self.dag_store.edge_set.add((self.token, child.token, len(child.parents) - 1))\n",
        "        if self.token is not None and child.token is not None:\n",
        "            self\\\n",
        "                .dag_store\\\n",
        "                .adjacency_matrix[self.dag_store.token_index_map[self.token], self.dag_store.token_index_map[child.token]] = 1\n",
        "\n",
        "\n",
        "    # since this is recursively saving smaller tokens, it's basically depth-first\n",
        "    def build_subgraph(self, suffix_node=None, suffix_tokenization=[]):\n",
        "      vertices = self.dag_store.vertices\n",
        "\n",
        "      for token in suffix_tokenization:\n",
        "\n",
        "        # if the predecessor token is not in the vertex store,\n",
        "        #   recursively build a sub-graph of suffix tokens\n",
        "        if token not in vertices.keys():\n",
        "            # create a new dag node for the current token\n",
        "            #   and put it in the vertex store\n",
        "            vertices[token] = CompositionDAGNode(token=token,\n",
        "                                                 frequency=suffix_node\\\n",
        "                                                              .flat_tree_store\\\n",
        "                                                              .child_dict[token]\\\n",
        "                                                              .frequency,\n",
        "                                                 dag_store = self.dag_store)\n",
        "\n",
        "            # break the missing token into even smaller tokens using the largest available smaller tokens\n",
        "            curr_suffix_tokenization = suffix_node.flat_tree_store.tokenize(text=token,\n",
        "                                                max_token_len=len(token) - 1)\n",
        "            # build a subgraph from the smaller tokens\n",
        "            temp_vert = vertices[token]\n",
        "            temp_vert, additional_vertices = temp_vert.build_subgraph(suffix_node, curr_suffix_tokenization)\n",
        "            vertices.update(additional_vertices)\n",
        "\n",
        "            vertices[token] = temp_vert\n",
        "\n",
        "        # base case: if the predecessor is in the vertex store\n",
        "        #   add an edge from the current node's predecessor to it\n",
        "        vertices[token].add_edge(self)\n",
        "\n",
        "      return self, vertices\n",
        "\n",
        "\n",
        "    # do breadth-first accumulation of the suffix tree into the dag\n",
        "    def suffix_tree_to_dag(self, suffix_tree):\n",
        "        print(\"Building DAG from modified suffix tree...\")\n",
        "\n",
        "        all_tokens = suffix_tree.get_tokens()\n",
        "        # create a dict for mapping tokens to indices in the adjacency matrix\n",
        "        token_list = list(all_tokens)\n",
        "        self.dag_store.token_index_map = {token_list[i]: i for i in range(len(token_list))}\n",
        "        self.dag_store.reversed_token_map = {v: k for k, v in self.dag_store.token_index_map.items()}\n",
        "        # initialize the adjacency matrix in LIL format\n",
        "        #   for more efficient memory usage during composition and later modifications\n",
        "        self.dag_store.adjacency_matrix = sp.lil_matrix((len(all_tokens), len(all_tokens)))\n",
        "\n",
        "        # print(\"lil adjacency matrix:\\n\", self.dag_store.adjacency_matrix)\n",
        "\n",
        "        vertices = self.dag_store.vertices\n",
        "        vertices[self.token] = self\n",
        "\n",
        "        suffix_node_queue = Queue()\n",
        "        suffix_node_queue.put(suffix_tree)\n",
        "\n",
        "        while not suffix_node_queue.empty():\n",
        "            # get the next suffix node from the queue\n",
        "            current_suffix_node = suffix_node_queue.get()\n",
        "\n",
        "            if current_suffix_node.token is not None and current_suffix_node.token not in all_tokens:\n",
        "                raise KeyError(\"current_suffix_node.token not in all_tokens\")\n",
        "\n",
        "            # create a dag vertex and add it to the set of vertices\n",
        "            vert = CompositionDAGNode(token=current_suffix_node.token,\n",
        "                                      frequency=current_suffix_node.frequency,\n",
        "                                      dag_store = self.dag_store)\n",
        "            vertices[vert.token] = vert\n",
        "\n",
        "            # if it's the root of the base dag or one of the top-level tokens, just add it to the vertex dict\n",
        "            if current_suffix_node.parent is None or current_suffix_node.parent.token is None:\n",
        "                vertices[current_suffix_node.token] = vert\n",
        "                # add an edge from the base graph's root to the top-level token\n",
        "                self.add_edge(vert)\n",
        "\n",
        "            # otherwise, add edges\n",
        "            else:\n",
        "                # tokenize the current token using the largest available smaller tokens\n",
        "                current_tokenization = current_suffix_node.flat_tree_store.tokenize(current_suffix_node.token,\n",
        "                                                                    len(current_suffix_node.token) - 1)\n",
        "\n",
        "                temp_vert = vert\n",
        "                temp_vert, additional_vertices = temp_vert.build_subgraph(current_suffix_node, current_tokenization)\n",
        "                vertices[vert.token] = temp_vert\n",
        "                vertices.update(additional_vertices)\n",
        "\n",
        "            # add all the current node's children to the queue\n",
        "            for child_token in current_suffix_node.keys_to_my_children:\n",
        "                suffix_node_queue.put(current_suffix_node.flat_tree_store.child_dict[child_token])\n",
        "\n",
        "        # print(\"lil djacency matrix after processing: \", self.dag_store.adjacency_matrix)\n",
        "\n",
        "        # convert the LIL adjacency matrix to CSR format for more efficient modification\n",
        "        self.dag_store.adjacency_matrix = sp.csr_matrix(self.dag_store.adjacency_matrix)\n",
        "\n",
        "        # print(\"Sparse adjacency matrix:\\n\", self.dag_store.adjacency_matrix)\n",
        "\n",
        "        self.dag_store.edge_set = {(pre, cum, pos) for pre, cum, pos in self.dag_store.edge_set if pre is not None}"
      ],
      "metadata": {
        "id": "Jnl1pI1qM2Yl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "8Z3kELrESdur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables and functions"
      ],
      "metadata": {
        "id": "5tV0r9uAMCLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_url = \"https://courses.cs.washington.edu/courses/cse163/20wi/files/lectures/L04/bee-movie.txt\"\n",
        "# with url.urlopen(test_url) as f:\n",
        "#     text = f.read().decode('utf-8')\n",
        "# # previously 454:500\n",
        "# text = text[454:500]\n",
        "# # text = text[0:500]\n",
        "text = \"black. Yellow, black.\\n :\\nOoh, black and yellow\"\n",
        "text = \"abbabababba yogabbagabba\"\n",
        "tests = [text]\n",
        "\n",
        "print(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SscW6f95ZqeP",
        "outputId": "4c443a83-2151-47e8-8b5e-dc28017a3df3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abbabababba yogabbagabba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Unit Tests"
      ],
      "metadata": {
        "id": "GZPBXmWNwFov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SuffixTests(unittest.TestCase):\n",
        "  def testTreeComposition():\n",
        "    text == \"abbabababba\"\n",
        "\n",
        "    suffix_tree, tokenizations[(text, min_freq)] = get_suffix_tree(text,\n",
        "                                                                 min_freq=2,\n",
        "                                                                 delimiters={\" \", \"\\n\"},\n",
        "                                                                 tree=None,\n",
        "                                                                 test=True)\n",
        "    token_set = set(suffix_tree.flat_tree_store.child_dict.keys())\n",
        "    base_token_set = {'bba', '', 'abababba', 'b', 'abba', 'ba', 'baa'}\n",
        "    print(token_set, base_token_set)\n",
        "    self.assertEqual(token_set, base_token_set)"
      ],
      "metadata": {
        "id": "_1-vcvwKwIX_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing function"
      ],
      "metadata": {
        "id": "lF-2boybMn8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test(text,\n",
        "             min_freq,\n",
        "             delimiters=None,\n",
        "             tree=None,\n",
        "             parallelize=True):\n",
        "  suffix_tree, tokenizations[(text, min_freq)] = get_suffix_tree(text,\n",
        "                                                                 min_freq,\n",
        "                                                                 delimiters=delimiters,\n",
        "                                                                 tree=tree,\n",
        "                                                                 parallelize=parallelize)\n",
        "  # print(\"modified suffix tree composed in \", time.time() - start_time, \" seconds.\")\n",
        "  # print_tree(suffix_tree)\n",
        "\n",
        "  composition_dag = CompositionDAGNode()\n",
        "  # start_time = time.time()\n",
        "  composition_dag.suffix_tree_to_dag(suffix_tree)\n",
        "  # end_time = time.time() - start_time\n",
        "  # print(\"dag composed in \", end_time, \" seconds.\")\n",
        "\n",
        "  if (min_freq % int(max(freq_range)/num_graphs_to_plot) == 0):\n",
        "      plot_dag(composition_dag.dag_store,\n",
        "               A=composition_dag.dag_store.adjacency_matrix,\n",
        "               k=4,\n",
        "               scaling=25)\n",
        "\n",
        "  # get tensor embeddings for all vertices\n",
        "  start_time = time.time()\n",
        "  token_vector_mappings = vectorize(composition_dag.dag_store.adjacency_matrix,\n",
        "                                    composition_dag.dag_store.reversed_token_map,\n",
        "                                    tokenizations[(text, min_freq)])\n",
        "  end_time = time.time() - start_time\n",
        "\n",
        "  # return {(pre, cum, pos+1) for pre, cum, pos in composition_dag.dag_store.edge_set if pre is not None}\n",
        "  return end_time, suffix_tree, token_vector_mappings"
      ],
      "metadata": {
        "id": "w3R0E36ZyPZ4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run tests"
      ],
      "metadata": {
        "id": "ytUGfTCFMLsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unittest.main()"
      ],
      "metadata": {
        "id": "rbAOb13axh7o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev_trees = dict()\n",
        "\n",
        "for min_freq in freq_range:\n",
        "  print(\"minimum frequency: \", min_freq)\n",
        "\n",
        "  for i in range(len(tests)):\n",
        "    # print(\"test \", i)\n",
        "    # if there's no previous tree stored for this test\n",
        "    if i not in prev_trees.keys():\n",
        "      prev_trees[i] = None\n",
        "\n",
        "    mean_time = 0\n",
        "    for fold in range(folds):\n",
        "      new_time=0\n",
        "      new_time, prev_trees[i], token_vector_mappings = run_test(text=tests[i],\n",
        "                                                        min_freq=min_freq,\n",
        "                                                        delimiters=delimiters,\n",
        "                                                        tree=prev_trees[i],\n",
        "                                                        parallelize=parallelize_suffix_tree_composition)\n",
        "      mean_time += new_time\n",
        "\n",
        "    plot_embeddings(token_vector_mappings, max_vector_plots)\n",
        "\n",
        "    test_results[\"min frequency\"].append(min_freq)\n",
        "    test_results[\"test number\"].append(i)\n",
        "    test_results[\"mean time\"].append(mean_time/folds)\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "imsuKapecmdG",
        "outputId": "b2c17df6-7b3d-443c-e5a2-e6f326fb96ad"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum frequency:  2\n",
            "Building modified suffix tree...\n",
            "Running suffix_tree.base_build_tree()...\n",
            "Initial suffix tree (just alphabet):\n",
            "{<__main__.SuffixNode object at 0x78492db95870>, <__main__.SuffixNode object at 0x78492db94a90>, <__main__.SuffixNode object at 0x78492db942b0>, <__main__.SuffixNode object at 0x78492db944c0>, <__main__.SuffixNode object at 0x78492db94bb0>, <__main__.SuffixNode object at 0x78492db949d0>}\n",
            "g: 0\n",
            "set()\n",
            " : 0\n",
            "set()\n",
            "b: 0\n",
            "set()\n",
            "o: 0\n",
            "set()\n",
            "y: 0\n",
            "set()\n",
            "a: 0\n",
            "set()\n",
            "Building suffix tree for 'abbabababba'...\n",
            "Current suffix: 'a'\n",
            "Children: {'o', 'y', 'a', 'b', ' ', 'g'}\n",
            "Child with shared suffix: 'a'\n",
            "Current suffix: 'ba'\n",
            "Children: {'o', 'y', 'a', 'b', ' ', 'g'}\n",
            "Child with shared suffix: 'b'\n",
            "Current suffix: 'a'\n",
            "Children: set()\n",
            "Creating new child...\n",
            "\n",
            "Child token set: set()\n",
            "set()\n",
            "Child token set after adding child to set: {'ba'}\n",
            "\n",
            "\n",
            "Current suffix: 'bba'\n",
            "Children: {'o', 'y', 'a', 'b', ' ', 'g'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'o'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-5aed8c6117a7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mnew_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       new_time, prev_trees[i], token_vector_mappings = run_test(text=tests[i],\n\u001b[0m\u001b[1;32m     16\u001b[0m                                                         \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                         \u001b[0mdelimiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d22f8c74ec49>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(text, min_freq, delimiters, tree, parallelize)\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mtree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              parallelize=True):\n\u001b[0;32m----> 6\u001b[0;31m   suffix_tree, tokenizations[(text, min_freq)] = get_suffix_tree(text,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                                                  \u001b[0mmin_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                  \u001b[0mdelimiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-76511cd030b5>\u001b[0m in \u001b[0;36mget_suffix_tree\u001b[0;34m(text, threshold, delimiters, tree, parallelize)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0msuffix_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_tree_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuffix_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       suffix_tree.base_build_tree(text,\n\u001b[0m\u001b[1;32m     38\u001b[0m                                   \u001b[0mdelimiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                   delimiter_regex=delimiter_regex)\n",
            "\u001b[0;32m<ipython-input-104-e372f62a6ad8>\u001b[0m in \u001b[0;36mbase_build_tree\u001b[0;34m(self, text, delimiters, delimiter_regex)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# add the suffix to the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;31m# if debugging and verbose[\"SuffixNode\"]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-e372f62a6ad8>\u001b[0m in \u001b[0;36madd_suffix\u001b[0;34m(self, suffix)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Iterate over each child to find the longest common prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys_to_my_children\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_tree_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Find the longest common prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'o'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests_df = pd.DataFrame.from_dict(test_results)\n",
        "tests_df.head()"
      ],
      "metadata": {
        "id": "amIeLCRJFLtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(tests_df[\"min frequency\"],\n",
        "          tests_df[\"mean time\"]\n",
        ")\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.xlabel('Min Frequency (log scale)')\n",
        "plt.ylabel('Mean Time (log scale)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LJEfM6CFHgcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6itywsKXnu2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
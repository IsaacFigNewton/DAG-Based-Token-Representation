{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RWgUeFn-XoEf",
        "Y6mne35jN5LZ",
        "_k3t-Px1Mo4D",
        "v3vtrUYGMsr0",
        "RYBfpRS1-yAs",
        "IPlDRvdHNJjE",
        "vNhEGAtBxk_z",
        "P7XfINmnDAfK",
        "5tV0r9uAMCLl",
        "lF-2boybMn8d",
        "t5jAIsElMq40"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/DAG-Based-Tokenization/blob/recreating-wordpiece/DAG_Text_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "1.  Refactor the suffix tree composition algorithm to take a divide-and-conquer aka a split-and-merge approach using multithreading\n",
        "\n",
        "Bring suffix tree composition complexity further down to logn by splitting on middle\n",
        "most spaces into sub tasks\n",
        "\n",
        "2.  Represent vectorization of text as fourth dimension, with token vector scaling corresponding to token frequencies or paths\n",
        "3.  Create token distance tensor during dag mapping for added accuracy"
      ],
      "metadata": {
        "id": "ky2XwlYkVi4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages and import libraries"
      ],
      "metadata": {
        "id": "RWgUeFn-XoEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow"
      ],
      "metadata": {
        "id": "sLDhGaKUQuxE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wpoJAKxqGmT8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "from threading import Thread\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import math\n",
        "import random\n",
        "from queue import Queue\n",
        "import numpy as np\n",
        "import urllib.request as url\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Important functions"
      ],
      "metadata": {
        "id": "xlFwktSL-2gM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes"
      ],
      "metadata": {
        "id": "gp8TIrvQG243"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage"
      ],
      "metadata": {
        "id": "Y6mne35jN5LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DAGStore:\n",
        "  def __init__(self,\n",
        "               vertices=None,\n",
        "               edge_set=None,\n",
        "               token_index_map=None,\n",
        "               adjacency_matrix=None):\n",
        "\n",
        "      if vertices is None:\n",
        "        vertices = dict()\n",
        "      if edge_set is None:\n",
        "        edge_set = set()\n",
        "\n",
        "      self.vertices = vertices\n",
        "      self.edge_set = edge_set\n",
        "      self.token_index_map = token_index_map\n",
        "      self.reversed_token_map = None\n",
        "      # first dimension = outgoing token node\n",
        "      # second dimension = incoming token node\n",
        "      self.adjacency_matrix = adjacency_matrix"
      ],
      "metadata": {
        "id": "MBQh8GVFOCOi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SuffixNode Class\n",
        "Let:\n",
        "*    n=text length\n",
        "*    b=mean block size\n",
        "*    l=mean inter-block lexical diversity (l = n/b if every block has a unique suffix tree and l < n/b if some blocks have shared suffix tree compositions)\n",
        "\n",
        "<br>\n",
        "\n",
        "Blocked Ukkonen time complexity: O(n)\n",
        "*   If split with preprocessing (O(n) time), parallelization in a divide-and-conquer approach would offer maximum time complexity of O(b*l)\n",
        "\n",
        "\n",
        "\n",
        "pruning time complexity: O(log(b*l))\n",
        "\n",
        "tokenization time complexity: O(log(b*l))"
      ],
      "metadata": {
        "id": "_k3t-Px1Mo4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlatTreeStore:\n",
        "    def __init__(self, child_dict=None):\n",
        "\n",
        "        # a flattened tree of nodes,\n",
        "        #   so that a node's tree membership can be checked more easily\n",
        "        if child_dict is None:\n",
        "            child_dict = dict()\n",
        "        self.child_dict = child_dict"
      ],
      "metadata": {
        "id": "qhqOm0Nd8Kib"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SuffixNode:\n",
        "    def __init__(self,\n",
        "                 suffix=None,\n",
        "                 token=None,\n",
        "                 frequency=0,\n",
        "                 parent=None,\n",
        "                 keys_to_my_children=None,\n",
        "                 flat_tree_store=None):\n",
        "        self.suffix = suffix\n",
        "        self.token = token\n",
        "        self.frequency = frequency\n",
        "\n",
        "        self.parent = parent\n",
        "\n",
        "        # store tokens representing keys to children in the child_dict\n",
        "        if keys_to_my_children is None:\n",
        "            keys_to_my_children = set()\n",
        "        self.keys_to_my_children = keys_to_my_children\n",
        "\n",
        "        # store the child_dict separately\n",
        "        if flat_tree_store is None:\n",
        "            flat_tree_store = FlatTreeStore()\n",
        "        self.flat_tree_store = flat_tree_store\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.suffix\n",
        "\n",
        "\n",
        "    def print_tree(self, indent=4):\n",
        "      my_children = {self.flat_tree_store.child_dict[key] for key in self.keys_to_my_children}\n",
        "\n",
        "      # Iterate over the child.suffixes (features) in the tree\n",
        "      for child in my_children:\n",
        "          print(' ' * indent + str(child.suffix) + \": \" + str(child.frequency))\n",
        "          # If the child is a SuffixNode, recursively print the subtree\n",
        "          if isinstance(child, SuffixNode):\n",
        "              child.print_tree(indent=indent + 4)\n",
        "          else:\n",
        "              print(' ' * (indent + 4) + \"\\\"\" + str(child.suffix) + \"\\\": \" + str(child.frequency))\n",
        "\n",
        "\n",
        "    def set_token(self):\n",
        "      if self.parent.token:\n",
        "        self.token =  self.suffix + self.parent.token\n",
        "      else:\n",
        "        self.token = self.suffix\n",
        "\n",
        "\n",
        "    def get_tree_in_node(self, child):\n",
        "      old_suffix = child.suffix[i:]\n",
        "      new_suffix = \"\"\n",
        "      if self.suffix:\n",
        "          new_suffix = self.suffix[i:]\n",
        "\n",
        "      # Replace the old entry for the current node with a new one for the edge split\n",
        "      split_node = SuffixNode(suffix=child.suffix[:i],\n",
        "                              frequency=child.frequency,\n",
        "                              parent=self,\n",
        "                              flat_tree_store=self.flat_tree_store)\n",
        "      split_node.set_token()\n",
        "\n",
        "      # Create a new node for the new suffix\n",
        "      #     and move both it and the previous suffix beneath the split node\n",
        "      old_child = SuffixNode(\n",
        "                    suffix=new_suffix,\n",
        "                    frequency=1,\n",
        "                    parent=split_node,\n",
        "                    flat_tree_store=self.flat_tree_store)\n",
        "      old_child.set_token()\n",
        "\n",
        "      # Create a new node for the existing edge suffix\n",
        "      new_child = SuffixNode(suffix=old_suffix,\n",
        "                              frequency=1,\n",
        "                              parent=split_node,\n",
        "                              flat_tree_store=self.flat_tree_store,\n",
        "                              keys_to_my_children=child.keys_to_my_children)\n",
        "      new_child.set_token()\n",
        "\n",
        "      return split_node, old_child, new_child\n",
        "\n",
        "\n",
        "    def split_edge(self, child, i):\n",
        "      split_node, old_child, new_child = self.get_tree_in_node(child)\n",
        "\n",
        "      # Remove the old child\n",
        "      self.keys_to_my_children.remove(child.token)\n",
        "      del self.flat_tree_store.child_dict[child.token]\n",
        "\n",
        "      # Add the new split node\n",
        "      self.keys_to_my_children.add(child.token)\n",
        "      self.flat_tree_store.child_dict[child.token] = child\n",
        "\n",
        "      # Transfer the children to the new split node's child\n",
        "      for grandchild_key in child.keys_to_my_children:\n",
        "          split_node.flat_tree_store.child_dict[grandchild_key].parent = new_child\n",
        "\n",
        "      split_node.keys_to_my_children = {new_child.token, old_child.token}\n",
        "      self.flat_tree_store.child_dict[new_child.token] = new_child\n",
        "      self.flat_tree_store.child_dict[old_child.token] = old_child\n",
        "\n",
        "\n",
        "    def add_suffix(self, suffix):\n",
        "      my_children = {self.flat_tree_store.child_dict[key] for key in self.keys_to_my_children}\n",
        "\n",
        "      # Find the longest prefix match in the children\n",
        "      for child in my_children:\n",
        "\n",
        "          # Find the index of the longest common prefix\n",
        "          i = 0\n",
        "          while i < len(child.suffix) and i < len(suffix) and child.suffix[i] == suffix[i]:\n",
        "              i += 1\n",
        "\n",
        "          # If there is a common prefix\n",
        "          if i > 0:\n",
        "              # Update the frequency of the current node\n",
        "              child.frequency += 1\n",
        "\n",
        "              # If the common prefix is the entire child key, recurse into that child\n",
        "              if i == len(child.suffix):\n",
        "                  child.add_suffix(suffix[i:])\n",
        "\n",
        "              # If the common prefix is only part of the child key, split the edge\n",
        "              elif i < len(suffix):\n",
        "                  self.split_edge(child, i)\n",
        "\n",
        "              return\n",
        "\n",
        "      # No matching prefix, add the suffix as a new child\n",
        "      new_child = SuffixNode(suffix=suffix,\n",
        "                                    frequency=1,\n",
        "                                    parent=self)\n",
        "      new_child.set_token()\n",
        "\n",
        "      self.keys_to_my_children.add(new_child.token)\n",
        "      self.flat_tree_store.child_dict[new_child.token] = new_child\n",
        "\n",
        "\n",
        "    def merge_trees(self, other):\n",
        "        other_tree = other.flat_tree_store.child_dict\n",
        "\n",
        "        shared_tokens = set(self.keys_to_my_children).intersection(set(other.keys_to_my_children))\n",
        "        other_unique_tokens = set(other.keys_to_my_children).difference(set(self.keys_to_my_children))\n",
        "\n",
        "        # combine children held in common between both suffix trees\n",
        "        for token in shared_tokens:\n",
        "            my_child = self.flat_tree_store.child_dict[token]\n",
        "            other_child = other_tree[token]\n",
        "\n",
        "            # combine the childrens' frequencies\n",
        "            self.flat_tree_store.child_dict[token].frequency += other_child.frequency\n",
        "\n",
        "            # recursively merge any grandchildren that might be held in common\n",
        "            self.flat_tree_store.child_dict[token].merge_trees(other_child)\n",
        "\n",
        "        # add the other tree's unique children to this tree\n",
        "        for token in other_unique_tokens:\n",
        "            other_child = other_tree[token]\n",
        "\n",
        "            other_child.parent = self\n",
        "            self.flat_tree_store.child_dict[other_child.token] = other_child\n",
        "\n",
        "\n",
        "    def find_split_point(text, delimiters):\n",
        "        if not delimiters:\n",
        "            return len(text) // 2\n",
        "\n",
        "        # Find all matches for the delimiter regex\n",
        "        matches = list(re.finditer(delimiters, text))\n",
        "\n",
        "        if not matches:\n",
        "            return None\n",
        "\n",
        "        # Find the match closest to the middle of the string\n",
        "        mid = len(text) // 2\n",
        "        best_match = min(matches, key=lambda m: abs(m.start() - mid))\n",
        "\n",
        "        return best_match.start()\n",
        "\n",
        "\n",
        "    def base_build_tree(self, text=\"\", delimiters=None):\n",
        "        # split the text into blocks, where each block is an independent clause\n",
        "        clauses = re.split(delimiters, text)\n",
        "\n",
        "        for string in clauses:\n",
        "            # loop through the string, starting with the last character\n",
        "            for i in range(0, len(string)):\n",
        "                suffix = string[len(string) - i - 1:]\n",
        "\n",
        "                # add the suffix to the tree\n",
        "                self.add_suffix(suffix)\n",
        "\n",
        "\n",
        "    def parallelized_build_tree(text, delimiters=None):\n",
        "        if len(text) == 0:\n",
        "            return SuffixNode()\n",
        "\n",
        "        # Find the best split point using delimiters\n",
        "        split_point = SuffixNode.find_split_point(text, delimiters)\n",
        "\n",
        "        # if the text has been split down to the size of a word\n",
        "        if split_point == None or split_point == 0 or split_point == len(text)-1:\n",
        "            root = SuffixNode()\n",
        "\n",
        "            # loop through the string, starting with the last character\n",
        "            for i in range(0, len(text)):\n",
        "                suffix = text[len(text) - i - 1:]\n",
        "\n",
        "                # add the suffix to the tree\n",
        "                root.add_suffix(suffix)\n",
        "\n",
        "            return root\n",
        "\n",
        "        left_half = text[:split_point]\n",
        "        right_half = text[split_point:]\n",
        "\n",
        "        left_tree = SuffixNode.build_tree(left_half, delimiters)\n",
        "        right_tree = SuffixNode.build_tree(right_half, delimiters)\n",
        "\n",
        "        left_tree.merge_trees(right_tree)\n",
        "\n",
        "        return left_tree\n",
        "\n",
        "\n",
        "    def prune_tree(self, threshold=2):\n",
        "        # If the node has no children (ie it's a leaf), return\n",
        "        if not self.keys_to_my_children:\n",
        "            # print(\"no children\")\n",
        "            return\n",
        "\n",
        "        children_to_kill = set()\n",
        "        # Recursively prune the tree\n",
        "        for child_token in self.keys_to_my_children:\n",
        "            child = self.flat_tree_store.child_dict[child_token]\n",
        "\n",
        "            if isinstance(child, SuffixNode):\n",
        "                # if the child is above the threshold or it's a single character token node\n",
        "                if child.frequency >= threshold or child.parent is None:\n",
        "                    # print(\"not removed\")\n",
        "                    self.flat_tree_store.child_dict[child_token].prune_tree(threshold)\n",
        "                else:\n",
        "                    # print(\"removed\")\n",
        "                    children_to_kill.add(child_token)\n",
        "\n",
        "        for child_token in children_to_kill:\n",
        "            # if the token's frequency falls below the threshold, prune it\n",
        "            self.keys_to_my_children.remove(child_token)\n",
        "            del self.flat_tree_store.child_dict[child_token]\n",
        "\n",
        "\n",
        "    # set all the suffix tree nodes' token properties\n",
        "    # return an aggregated set of all the tokens\n",
        "    def get_tokens(self, prev_token=\"\"):\n",
        "        tokens = dict()\n",
        "\n",
        "        for child_token in self.keys_to_my_children:\n",
        "            child = self.flat_tree_store.child_dict[child_token]\n",
        "\n",
        "            if isinstance(child, SuffixNode):\n",
        "                token = prev_token + child.suffix\n",
        "\n",
        "                # set the child's token for later use\n",
        "                self.flat_tree_store.child_dict[child_token].token = token\n",
        "\n",
        "                # add the accumulation of the current suffix with previous suffixes\n",
        "                #   as a new token in the token set, with the previously found frequency\n",
        "                tokens[token] = child.frequency\n",
        "                # Recursively get tokens from the child\n",
        "                tokens.update(child.get_tokens(token))\n",
        "\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "rovuxbqdXhDu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suffix_tree(text,\n",
        "                      threshold,\n",
        "                      delimiters=None,\n",
        "                      tree=None,\n",
        "                      test=True):\n",
        "  # use the tree option in case a previous tree is to be pruned further\n",
        "  suffix_tree = tree\n",
        "  if suffix_tree is None:\n",
        "    print(\"Building modified suffix tree...\")\n",
        "\n",
        "    # suffix_tree = SuffixNode(children = {SuffixNode(suffix=unique_char) for unique_char in set(text)})\n",
        "    # create a store for the tree nodes\n",
        "    flat_tree_store = FlatTreeStore(child_dict={\n",
        "        letter: SuffixNode(suffix=letter, token=letter) for letter in set(text)\n",
        "    })\n",
        "    # point all the internal tree stores back at the main one\n",
        "    for key in flat_tree_store.child_dict.keys():\n",
        "      flat_tree_store.child_dict[key].flat_tree_store = flat_tree_store\n",
        "    if test:\n",
        "      suffix_tree = SuffixNode(flat_tree_store=flat_tree_store,\n",
        "                               keys_to_my_children=set(flat_tree_store.child_dict.keys()))\n",
        "      suffix_tree.base_build_tree(text, delimiters=delimiters)\n",
        "    else:\n",
        "      suffix_tree = SuffixNode.parallelized_build_tree(text, delimiters=delimiters)\n",
        "\n",
        "    suffix_tree.print_tree()\n",
        "\n",
        "  print(\"Prunining modified suffix tree...\")\n",
        "  suffix_tree.prune_tree(threshold=threshold)\n",
        "  suffix_tree.print_tree()\n",
        "\n",
        "  print(\"Getting token set...\")\n",
        "  tokens = suffix_tree.get_tokens()\n",
        "  print(tokens)\n",
        "\n",
        "  return suffix_tree, tokens"
      ],
      "metadata": {
        "id": "FHLSheWoyikQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CompositionDAGNode\n",
        "construction time complexity: O(log(|V| + |E|))"
      ],
      "metadata": {
        "id": "v3vtrUYGMsr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CompositionDAGNode:\n",
        "    def __init__(self,\n",
        "                 token=None,\n",
        "                 frequency=0,\n",
        "                 parents=None,\n",
        "                 flat_tree_store=None,\n",
        "                 dag_store=None):\n",
        "        self.token = token\n",
        "        self.frequency = frequency\n",
        "\n",
        "        if parents is None:\n",
        "            parents = list()\n",
        "        self.parents = parents\n",
        "\n",
        "        if flat_tree_store is None:\n",
        "            flat_tree_store = FlatTreeStore()\n",
        "        self.flat_tree_store = flat_tree_store\n",
        "\n",
        "        if dag_store is None:\n",
        "            dag_store = DAGStore()\n",
        "        self.dag_store = dag_store\n",
        "\n",
        "    def __str__(self):\n",
        "        child_tokens = {child.token for child in self.flat_tree_store.child_dict.values()}\n",
        "        parent_tokens = {parent.token for parent in self.parents}\n",
        "        return f\"Token: {self.token}\\nParents: {parent_tokens}\\nChildren: {child_tokens}\"\n",
        "\n",
        "    # make an edge between the current token and a successor\n",
        "    def add_edge(self, child):\n",
        "        # Add the larger token as a child of this token\n",
        "        print(self.flat_tree_store.child_dict)\n",
        "        self.flat_tree_store.child_dict[child.token] = child\n",
        "\n",
        "        # Add this token to the list of parents composing the larger token\n",
        "        child.parents.append(self)\n",
        "\n",
        "        # Add an edge to the edge list, using the current token's position\n",
        "        #   in the child token as the edge weight\n",
        "        self.dag_store.edge_set.add((self.token, child.token, len(child.parents) - 1))\n",
        "        if self.token is not None and child.token is not None:\n",
        "            self\\\n",
        "                .dag_store\\\n",
        "                .adjacency_matrix[self.dag_store.token_index_map[self.token], self.dag_store.token_index_map[child.token]] = 1\n",
        "\n",
        "\n",
        "    # since this is recursively saving smaller tokens, it's basically depth-first\n",
        "    def build_subgraph(self, all_tokens, suffix_tokenization):\n",
        "      vertices = self.dag_store.vertices\n",
        "\n",
        "      for token in suffix_tokenization:\n",
        "\n",
        "        # if the predecessor token is not in the vertex store,\n",
        "        #   recursively build a sub-graph of suffix tokens\n",
        "        if token not in vertices.keys():\n",
        "            # create a new dag node for the current token\n",
        "            #   and put it in the vertex store\n",
        "            vertices[token] = CompositionDAGNode(token=token,\n",
        "                                                 frequency=all_tokens[token],\n",
        "                                                 dag_store = self.dag_store)\n",
        "\n",
        "            # break the missing token into even smaller tokens using the largest available smaller tokens\n",
        "            curr_suffix_tokenization = tokenize(token, all_tokens, len(token) - 1)\n",
        "            # build a subgraph from the smaller tokens\n",
        "            temp_vert = vertices[token]\n",
        "            temp_vert, additional_vertices = temp_vert.build_subgraph(all_tokens, curr_suffix_tokenization)\n",
        "            vertices.update(additional_vertices)\n",
        "\n",
        "            vertices[token] = temp_vert\n",
        "\n",
        "        # base case: if the predecessor is in the vertex store\n",
        "        #   add an edge from the current node's predecessor to it\n",
        "        vertices[token].add_edge(self)\n",
        "\n",
        "      return self, vertices\n",
        "\n",
        "\n",
        "    # do breadth-first accumulation of the suffix tree into the dag\n",
        "    def suffix_tree_to_dag(self, suffix_tree):\n",
        "        print(\"Building DAG from modified suffix tree...\")\n",
        "\n",
        "        all_tokens = suffix_tree.get_tokens()\n",
        "        # create a dict for mapping tokens to indices in the adjacency matrix\n",
        "        token_list = list(all_tokens.items())\n",
        "        self.dag_store.token_index_map = {token_list[i][0]: i for i in range(len(token_list))}\n",
        "        self.dag_store.reversed_token_map = {v: k for k, v in self.dag_store.token_index_map.items()}\n",
        "        # initialize the adjacency matrix in LIL format\n",
        "        #   for more efficient memory usage during composition and later modifications\n",
        "        self.dag_store.adjacency_matrix = sp.lil_matrix((len(all_tokens), len(all_tokens)))\n",
        "\n",
        "        print(\"lil adjacency matrix: \", self.dag_store.adjacency_matrix)\n",
        "\n",
        "        vertices = self.dag_store.vertices\n",
        "        vertices[self.token] = self\n",
        "\n",
        "        suffix_node_queue = Queue()\n",
        "        suffix_node_queue.put(suffix_tree)\n",
        "\n",
        "        while not suffix_node_queue.empty():\n",
        "            # get the next suffix node from the queue\n",
        "            current_suffix_node = suffix_node_queue.get()\n",
        "\n",
        "            # create a dag vertex and add it to the set of vertices\n",
        "            vert = CompositionDAGNode(token=current_suffix_node.token,\n",
        "                                      frequency=current_suffix_node.frequency,\n",
        "                                      dag_store = self.dag_store)\n",
        "            vertices[vert.token] = vert\n",
        "\n",
        "            # if it's the root of the base dag or one of the top-level tokens, just add it to the vertex dict\n",
        "            if current_suffix_node.parent is None or current_suffix_node.parent.token is None:\n",
        "                vertices[current_suffix_node.token] = vert\n",
        "                # add an edge from the base graph's root to the top-level token\n",
        "                self.add_edge(vert)\n",
        "\n",
        "            # otherwise, add edges\n",
        "            else:\n",
        "                # tokenize the current token using the largest available smaller tokens\n",
        "                current_tokenization = tokenize(current_suffix_node.token,\n",
        "                                               all_tokens,\n",
        "                                               len(current_suffix_node.token) - 1)\n",
        "\n",
        "                temp_vert = vert\n",
        "                temp_vert, additional_vertices = temp_vert.build_subgraph(all_tokens, current_tokenization)\n",
        "                vertices[vert.token] = temp_vert\n",
        "                vertices.update(additional_vertices)\n",
        "\n",
        "            # add all the current node's children to the queue\n",
        "            for child_token in current_suffix_node.keys_to_my_children:\n",
        "                suffix_node_queue.put(current_suffix_node.flat_tree_store.child_dict[child_token])\n",
        "\n",
        "        print(\"lil djacency matrix after processing: \", self.dag_store.adjacency_matrix)\n",
        "\n",
        "        # convert the LIL adjacency matrix to CSR format for more efficient modification\n",
        "        self.dag_store.adjacency_matrix = sp.csr_matrix(self.dag_store.adjacency_matrix)\n",
        "\n",
        "        print(\"Sparse adjacency matrix: \", self.dag_store.adjacency_matrix)\n",
        "\n",
        "        self.dag_store.edge_set = {(pre, cum, pos) for pre, cum, pos in self.dag_store.edge_set if pre is not None}"
      ],
      "metadata": {
        "id": "Jnl1pI1qM2Yl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utility"
      ],
      "metadata": {
        "id": "RYBfpRS1-yAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Operational"
      ],
      "metadata": {
        "id": "IPlDRvdHNJjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, token_dict, max_token_len):\n",
        "    # filter to include only tokens shorter than the\n",
        "    available_tokens = {token: freq for token, freq in token_dict.items() if len(token) <= max_token_len}\n",
        "\n",
        "    # sort tokens by length and secondarily frequency\n",
        "    best_tokens = sorted(list(available_tokens.items()),\n",
        "                         key=lambda x: (-len(x[0]), -x[1]))\n",
        "\n",
        "    # extract tokens from the ordered list\n",
        "    tokenization = []\n",
        "    while text != \"\":\n",
        "        for token, freq in best_tokens:\n",
        "            if text.startswith(token):\n",
        "                tokenization.append(token)\n",
        "                text = text[len(token):]\n",
        "\n",
        "    return tokenization"
      ],
      "metadata": {
        "id": "ywGwIKh2-Z8x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Embedding"
      ],
      "metadata": {
        "id": "vNhEGAtBxk_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distances_for_subgraph(labels, adjacency_matrix, subgraph_id):\n",
        "    \"\"\"\n",
        "    Calculate Manhattan distances for nodes in a specific subgraph and update the distance matrix.\n",
        "\n",
        "    Parameters:\n",
        "    labels (numpy array): Array of subgraph labels for each node.\n",
        "    adjacency_matrix (CSR matrix): A sparse adjacency matrix of the graph.\n",
        "    subgraph_id (int): ID of the subgraph to process.\n",
        "    \"\"\"\n",
        "    # extract a list of all the subgraph vertices in the current subgraph\n",
        "    subgraph_vertices = np.where(labels == subgraph_id)[0]\n",
        "\n",
        "    n = adjacency_matrix.shape[0]\n",
        "\n",
        "    indices = []\n",
        "    values = []\n",
        "    # for each seed node\n",
        "    for i in subgraph_vertices:\n",
        "        # for each outgoing node in the subgraph\n",
        "        for x in subgraph_vertices:\n",
        "          # for each incoming node in the subgraph\n",
        "          for y in subgraph_vertices:\n",
        "              # if the entry, when double-checked, is found to be nonzero\n",
        "              if adjacency_matrix[x, y] != 0 or i == x and x == y:\n",
        "                  # calculate inverted Manhattan distance, kind of\n",
        "                  #   since a node in the same row or column as the seed\n",
        "                  #   must have a distance of 1 from the seed,\n",
        "                  #   determine the shortest distance to either the row or column\n",
        "                  #   this is a heuristic since it would take too long to re-traverse the dag from scratch\n",
        "\n",
        "                  # if the entry represents the seed node\n",
        "                  if i == x and x == y:\n",
        "                    inverse_manhattan_distance = 1\n",
        "                  else:\n",
        "                    #   numerator is 0.75 to discount weight of non-self nodes\n",
        "                    inverse_manhattan_distance = 0.75 / np.maximum(min(np.abs(x - i), np.abs(y - i)), 1)\n",
        "\n",
        "                  # Store indices and values in lists\n",
        "                  indices.append([i, x, y])\n",
        "                  values.append(inverse_manhattan_distance)\n",
        "\n",
        "    if not indices:\n",
        "        print(\"Error, the entry for the seed node was not created in the tensor\")\n",
        "        return None\n",
        "\n",
        "    # return for recombination with the entire graph's tensor\n",
        "    return indices, values"
      ],
      "metadata": {
        "id": "01mgXVqdUbBO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_adjacency_matrix(adjacency_matrix, low_mem=True):\n",
        "    \"\"\"\n",
        "    Vectorize the adjacency matrix by calculating Manhattan distances for each subgraph.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_matrix (CSR matrix): Adjacency matrix of the graph.\n",
        "\n",
        "    Returns:\n",
        "    sparse CSR matrix: 3D distance matrix.\n",
        "    \"\"\"\n",
        "    n = adjacency_matrix.shape[0]\n",
        "\n",
        "    # Identify subgraphs and their labels\n",
        "    num_subgraphs, labels = connected_components(adjacency_matrix, directed=False, return_labels=True)\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    if low_mem:\n",
        "        # Calculate distances for each subgraph in series\n",
        "        for subgraph_id in range(num_subgraphs):\n",
        "            subgraph_distance_tensor = calculate_distances_for_subgraph(labels,\\\n",
        "                                                                        adjacency_matrix,\\\n",
        "                                                                        subgraph_id)\n",
        "\n",
        "            # if there's a subgraph distance tensor to combine the original with...\n",
        "            if subgraph_distance_tensor is not None:\n",
        "                # Recombine the partial vectorizations given by the subgraphs into\n",
        "                #   a single vectorization for the entire graph\n",
        "                subgraph_indices, subgraph_values = calculate_distances_for_subgraph(labels,\n",
        "                                                                                      adjacency_matrix,\n",
        "                                                                                      subgraph_id)\n",
        "                indices = indices + subgraph_indices\n",
        "                values = values + subgraph_values\n",
        "    else:\n",
        "        # Calculate distances for each subgraph in parallel\n",
        "        # Create and start threads for each subgraph\n",
        "        threads = []\n",
        "        # for subgraph_id in range(num_subgraphs):\n",
        "        #     thread = Thread(target=calculate_distances_for_subgraph,\n",
        "        #                     args=(labels,\n",
        "        #                           adjacency_matrix,\n",
        "        #                           subgraph_id))\n",
        "        #     thread.start()\n",
        "        #     threads.append(thread)\n",
        "\n",
        "        # # Wait for all threads to complete\n",
        "        # for thread in threads:\n",
        "        #     thread.join()\n",
        "        pass\n",
        "\n",
        "\n",
        "    # print(indices, values)\n",
        "\n",
        "    # Return a sparse tensor for storing the tokens' distance tensors/embeddings\n",
        "    return tf.sparse.SparseTensor(indices=indices,\n",
        "                                  values=values,\n",
        "                                  dense_shape=[n, n, n])"
      ],
      "metadata": {
        "id": "y7oPMU_HPRds"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_array(tensor):\n",
        "  return tf.sparse.to_dense(tensor).numpy()"
      ],
      "metadata": {
        "id": "utf7-Khb9C4C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_slice(tensor, slice_index):\n",
        "  # print(\"Getting slice for token \", slice_index)\n",
        "  indices = tensor.indices.numpy()\n",
        "  values = tensor.values.numpy()\n",
        "  n = tensor.dense_shape.numpy()[0]\n",
        "  new_indices = []\n",
        "  new_values = []\n",
        "\n",
        "  for i, index in enumerate(indices):\n",
        "    if index[0] == slice_index:\n",
        "      new_indices.append([index[1], index[2]])\n",
        "      new_values.append(values[i])\n",
        "\n",
        "  # print(new_indices, new_values)\n",
        "  return tf.sparse.SparseTensor(indices=new_indices,\n",
        "                                  values=new_values,\n",
        "                                  dense_shape=[n, n])"
      ],
      "metadata": {
        "id": "fxPyCBLD2ssj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(adjacency_matrix, reversed_token_map, token_set):\n",
        "  n = len(token_set)\n",
        "  token_tensor = vectorize_adjacency_matrix(adjacency_matrix)\n",
        "\n",
        "  # create a dictionary of all the tokens and their respective tensor embedding slices\n",
        "  # print(token_tensor)\n",
        "  token_vector_mappings = {reversed_token_map[i]: get_tensor_slice(token_tensor, i) for i in range(n)}\n",
        "\n",
        "  for token in token_set.keys():\n",
        "      tok_vect_tensor = token_vector_mappings[token]\n",
        "\n",
        "  # print(token_vector_mappings)\n",
        "\n",
        "  return token_vector_mappings"
      ],
      "metadata": {
        "id": "V2bv4PySphAC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting and Printing"
      ],
      "metadata": {
        "id": "P7XfINmnDAfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings(embeddings, max_plots):\n",
        "  if max_plots < 1:\n",
        "    return\n",
        "\n",
        "  i = 0\n",
        "  for token, vector in embeddings.items():\n",
        "    if (i % max(len(embeddings) * int(i/max_plots), 1) == 0):\n",
        "        print(\"Embedding for'\" + token + \"':\")\n",
        "        dense_vector = tensor_to_array(vector)\n",
        "        # print(\"embedding string: \", vector)\n",
        "\n",
        "\n",
        "        # Convert the vector to its dense representation and create the heatmap\n",
        "        sns.heatmap(dense_vector, annot=False, cmap='viridis')\n",
        "\n",
        "        # Display the heatmap\n",
        "        plt.show()\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "d7I3OmAALAiF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dag(dag_store, A=None, scaling=50, edge_width=1, k=2):\n",
        "    print(\"Adjacency matrix: \", A)\n",
        "\n",
        "    if A is None:\n",
        "        print(\"Adjacency matrix was empty/not defined\")\n",
        "\n",
        "    # Create a directed graph from the adjacency matrix\n",
        "    G = nx.from_numpy_array(A, create_using=nx.DiGraph)\n",
        "\n",
        "    # Relabel the token nodes\n",
        "    G = nx.relabel_nodes(G, dag_store.reversed_token_map)\n",
        "\n",
        "    # # Draw the graph without edge labels\n",
        "    # # Convert nodes to strings before calculating length\n",
        "    # nx.draw(G, with_labels=True, node_size=[scaling * len(str(node)) for node in G.nodes()],\n",
        "    #         width=edge_width, font_size=8)\n",
        "\n",
        "\n",
        "    # Calculate figure size based on the number of nodes\n",
        "    num_nodes = len(G.nodes)\n",
        "    num_edges = len(G.edges)\n",
        "    graph_size = (num_nodes) + (2 * num_edges)\n",
        "\n",
        "    figsize = graph_size * scaling/300\n",
        "    font_size = 2 + math.sqrt(scaling)/5\n",
        "\n",
        "    # Position nodes using the spring layout\n",
        "    pos = nx.spring_layout(G, seed=42, k=k/num_nodes)\n",
        "\n",
        "    # Calculate node sizes based on the length of the token text\n",
        "    node_sizes = [scaling * len(node) for node in G.nodes()]\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize), dpi=300)\n",
        "\n",
        "    # Draw nodes with sizes proportional to the length of their text\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes)\n",
        "\n",
        "    # Draw edges with widths based on edge weights\n",
        "    nx.draw_networkx_edges(G,\n",
        "                          pos,\n",
        "                          width=edge_width,\n",
        "                          arrowstyle='-|>',\n",
        "                          connectionstyle=\"arc3,rad=0.2\")\n",
        "\n",
        "    # Draw node labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=font_size, font_family=\"sans-serif\")\n",
        "\n",
        "    # # Draw edge weight labels\n",
        "    # edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "    # nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "    # Customize and show plot\n",
        "    ax = plt.gca()\n",
        "    ax.margins(0.08)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sHXlkuoDNxEd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "8Z3kELrESdur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables and functions"
      ],
      "metadata": {
        "id": "5tV0r9uAMCLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing function"
      ],
      "metadata": {
        "id": "lF-2boybMn8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test(text,\n",
        "             min_freq,\n",
        "             delimiters=None,\n",
        "             tree=None):\n",
        "  suffix_tree, tokenizations[(text, min_freq)] = get_suffix_tree(text,\n",
        "                                                                 min_freq,\n",
        "                                                                 delimiters=delimiters,\n",
        "                                                                 tree=tree,\n",
        "                                                                 test=True)\n",
        "  # print(\"modified suffix tree composed in \", time.time() - start_time, \" seconds.\")\n",
        "  # print_tree(suffix_tree)\n",
        "\n",
        "  composition_dag = CompositionDAGNode()\n",
        "  # start_time = time.time()\n",
        "  composition_dag.suffix_tree_to_dag(suffix_tree)\n",
        "  # end_time = time.time() - start_time\n",
        "  # print(\"dag composed in \", end_time, \" seconds.\")\n",
        "\n",
        "  if (min_freq % int(max(freq_range)/num_graphs_to_plot) == 0):\n",
        "      plot_dag(composition_dag.dag_store,\n",
        "               A=composition_dag.dag_store.adjacency_matrix,\n",
        "               k=4,\n",
        "               scaling=25)\n",
        "\n",
        "  # get tensor embeddings for all vertices\n",
        "  start_time = time.time()\n",
        "  token_vector_mappings = vectorize(composition_dag.dag_store.adjacency_matrix,\n",
        "                                    composition_dag.dag_store.reversed_token_map,\n",
        "                                    tokenizations[(text, min_freq)])\n",
        "  end_time = time.time() - start_time\n",
        "\n",
        "  # return {(pre, cum, pos+1) for pre, cum, pos in composition_dag.dag_store.edge_set if pre is not None}\n",
        "  return end_time, suffix_tree, token_vector_mappings"
      ],
      "metadata": {
        "id": "w3R0E36ZyPZ4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Variables"
      ],
      "metadata": {
        "id": "t5jAIsElMq40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freq_range = range(50, 1050, 50)\n",
        "freq_range = range(2, 12, 2)\n",
        "folds = 1\n",
        "num_graphs_to_plot = 10\n",
        "max_vector_plots = 2\n",
        "tokenizations = dict()\n",
        "test_results = {\n",
        "    \"min frequency\": [],\n",
        "    \"test number\": [],\n",
        "    \"mean time\": [],\n",
        "}\n",
        "# match whitespace of arbitrary length\n",
        "delimiters = r\"\\s+\"   #r\"\\n\",     #r\"\\n\\n|.\\n|\\)\\n|:|\\.\\.\\.\","
      ],
      "metadata": {
        "id": "Z7TfB_s2fVeW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_url = \"https://courses.cs.washington.edu/courses/cse163/20wi/files/lectures/L04/bee-movie.txt\"\n",
        "with url.urlopen(test_url) as f:\n",
        "    text = f.read().decode('utf-8')\n",
        "# previously 454:500\n",
        "text = text[400:500]\n",
        "# text = text[0:500]\n",
        "# text = \"abbabba\"\n",
        "\n",
        "tests = [text]\n",
        "\n",
        "print(text[:50])"
      ],
      "metadata": {
        "id": "13ofz24yHJ9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ffcb12-8ec2-40a5-8233-7342e656af13"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ng out a shirt)\n",
            "Yellow, black. Yellow, black.\n",
            "Yell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run tests"
      ],
      "metadata": {
        "id": "ytUGfTCFMLsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prev_trees = dict()\n",
        "\n",
        "for min_freq in freq_range:\n",
        "  print(\"minimum frequency: \", min_freq)\n",
        "\n",
        "  for i in range(len(tests)):\n",
        "    # print(\"test \", i)\n",
        "    # if there's no previous tree stored for this test\n",
        "    if i not in prev_trees.keys():\n",
        "      prev_trees[i] = None\n",
        "\n",
        "    mean_time = 0\n",
        "    for fold in range(folds):\n",
        "      new_time=0\n",
        "      new_time, prev_trees[i], token_vector_mappings = run_test(text=tests[i],\n",
        "                                                        min_freq=min_freq,\n",
        "                                                        delimiters=delimiters,\n",
        "                                                        tree=prev_trees[i])\n",
        "      mean_time += new_time\n",
        "\n",
        "    plot_embeddings(token_vector_mappings, max_vector_plots)\n",
        "\n",
        "    test_results[\"min frequency\"].append(min_freq)\n",
        "    test_results[\"test number\"].append(i)\n",
        "    test_results[\"mean time\"].append(mean_time/folds)\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "imsuKapecmdG",
        "outputId": "8d105cd9-41bf-496a-b7f7-1d7f6b63dd01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'freq_range' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-204e483b291d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprev_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmin_freq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minimum frequency: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'freq_range' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests_df = pd.DataFrame.from_dict(test_results)\n",
        "tests_df.head()"
      ],
      "metadata": {
        "id": "amIeLCRJFLtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(tests_df[\"min frequency\"],\n",
        "          tests_df[\"mean time\"]\n",
        ")\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.xlabel('Min Frequency (log scale)')\n",
        "plt.ylabel('Mean Time (log scale)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LJEfM6CFHgcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}